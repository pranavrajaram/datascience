{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Car Type",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pranavrajaram/datascience/blob/main/Car_Type.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###DO NOT EDIT THIS CODE\n",
        "################################################################################################################################\n",
        "import cv2\n",
        "import requests\n",
        "import urllib.request\n",
        "import urllib\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torchvision import datasets, transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# GPUs are 3x faster than CPU. Better to use if it is available \n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Define Loss Function\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "# This function returns the number of parameters in the model\n",
        "def num_params(model):\n",
        "  return sum([p.numel() for p in model.parameters()])\n",
        "\n",
        "# Define a Training Function. This function will: compute the forward pass, backpropagate,\n",
        "# update the weights, and repeat the steps for a given number of epochs. At each epoch, \n",
        "# it will output the training loss and test loss at every step\n",
        "def train(epochs, model, trainloader, testloader, optimizer, loss_function):\n",
        "  for epoch in range(epochs):\n",
        "    loss_epoch = np.array([])\n",
        "    train_correct, train_total = 0, 0\n",
        "    test_correct, test_total = 0, 0\n",
        "\n",
        "    for data, labels in trainloader:\n",
        "      # convert into GPU objects if needed\n",
        "      input_data = data.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      # forward pass\n",
        "      predict = model(input_data)\n",
        "      \n",
        "      # backward pass\n",
        "      loss = loss_function(predict, labels)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "\n",
        "      # update parameters (weights and biases)\n",
        "      optimizer.step()\n",
        "\n",
        "      # store progress\n",
        "      loss_epoch = np.append(loss_epoch, loss.item())\n",
        "\n",
        "    # evaluate test accuracy\n",
        "    for data, labels in testloader:\n",
        "      input_data = data.to(device)\n",
        "      labels = labels.to(device)\n",
        "      predict = model(input_data)\n",
        "      for i, out in enumerate(predict):\n",
        "        pred = torch.argmax(out)\n",
        "        if pred == labels[i]:\n",
        "          test_correct+=1\n",
        "        test_total+=1\n",
        "\n",
        "    test_accuracy = test_correct/test_total    \n",
        "  \n",
        "    print('epoch [{}/{}], training loss:{:.4f}, test accuracy:{:.4f}'.format(epoch+1, epochs, np.mean(loss_epoch), test_accuracy))\n",
        "################################################################################################################################"
      ],
      "metadata": {
        "id": "Cs0osaT3FFTO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download and load data\n",
        "batch_size = 512\n",
        "\n",
        "\n",
        "# download and transform train dataset\n",
        "train_loader = torch.utils.data.DataLoader(datasets.EMNIST('./emnist_data', split='mnist', download=True, train=True, transform=transforms.Compose([\n",
        "                                                transforms.ToTensor(), # first, convert image to PyTorch tensor\n",
        "                                                transforms.Normalize((0.1307,), (0.3081,)) # normalize inputs\n",
        "                                                ])), batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# download and transform test dataset\n",
        "test_loader = torch.utils.data.DataLoader(datasets.EMNIST('./emnist_data', split='mnist', download=True, train=False, transform=transforms.Compose([\n",
        "                                                              transforms.ToTensor(), # first, convert image to PyTorch tensor\n",
        "                                                              transforms.Normalize((0.1307,), (0.3081,)) # normalize inputs\n",
        "                                                          ])), batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "qAvcstFBFwZC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################\n",
        "####                #### \n",
        "#### EDIT THIS CELL ####\n",
        "####                ####\n",
        "########################\n",
        "\n",
        "\n",
        "learning_rate = 42e-4\n",
        "weight_decay = 10e-5\n",
        "n_epochs = 9\n",
        "\n",
        "# neural network\n",
        "class NeuralNetwork(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(NeuralNetwork, self).__init__()\n",
        "    ### Define Layers\n",
        "    self.c1 = nn.Conv2d(1, 3, 5)\n",
        "    self.L1 = nn.Linear(1728, 723)\n",
        "    self.L2 = nn.Linear(723, 98)\n",
        "    self.L3 = nn.Linear(98, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.c1(x)\n",
        "    x = F.relu(x)\n",
        "    x = torch.flatten(x,1)\n",
        "    x = self.L1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.L2(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.L3(x)\n",
        "    x = torch.sigmoid(x)\n",
        "    return x\n",
        "  \n",
        "# Every time you edit the neural network, you'll have to update this cell\n",
        "# Create model object\n",
        "model = NeuralNetwork().to(device)\n",
        "\n",
        "# Loads Adam optimizer, which implements a version of gradient descent\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)"
      ],
      "metadata": {
        "id": "kiTuALg5FHom"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(n_epochs, model, train_loader, test_loader, optimizer, loss_function)"
      ],
      "metadata": {
        "id": "kltTrzmfFi9m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d012d9c4-3f58-4829-eb85-a62d611c1f77"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch [1/9], training loss:1.5531, test accuracy:0.9458\n",
            "epoch [2/9], training loss:1.4974, test accuracy:0.9627\n",
            "epoch [3/9], training loss:1.4920, test accuracy:0.9672\n",
            "epoch [4/9], training loss:1.4867, test accuracy:0.9595\n",
            "epoch [5/9], training loss:1.4856, test accuracy:0.9644\n",
            "epoch [6/9], training loss:1.4828, test accuracy:0.9728\n",
            "epoch [7/9], training loss:1.4803, test accuracy:0.9716\n",
            "epoch [8/9], training loss:1.4798, test accuracy:0.9757\n",
            "epoch [9/9], training loss:1.4780, test accuracy:0.9715\n"
          ]
        }
      ]
    }
  ]
}